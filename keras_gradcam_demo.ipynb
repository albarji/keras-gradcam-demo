{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "# Pre-trained deep neural network usage and interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"img/timber_wolf.png\" style=\"height: 200px;\"></td>\n",
    "        <td><img src=\"img/platypus.png\" style=\"height: 200px;\"></td>\n",
    "        <td><img src=\"img/westie.png\" style=\"height: 200px;\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "In this tutorial notebook we will make use of a pre-trained Deep Convolutional Network to automatically detect a variety of objects in images. We will also make use of the interpretation technique Grad-CAM to understand why the neural network is making its decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will show all images inside the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a pre-trained neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by loading a pre-trained deep convolutional neural network: Xception. This network is an improved and more compact version of the 152-layers network that won the ImageNet Large Scale Visual Recognition Challenge in 2015. The network was trained to be able to recognize [1000 different classes of objects](http://image-net.org/challenges/LSVRC/2017/browse-synsets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.xception import Xception\n",
    "\n",
    "# Load the network configured to process 299x299 pixels colored images\n",
    "model = Xception(input_shape=(299, 299, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a description of the network structure as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the neural network to detect objects in an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, we will use an image of oranges from Wikipedia. The following code downloads the image and loads it as a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "!wget -O input.jpg \"https://upload.wikimedia.org/wikipedia/commons/b/b0/OrangeBloss_wb.jpg\"\n",
    "img = np.array(Image.open(\"./input.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize the downloaded image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image.fromarray(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function transforms the image into a tensor suitable for Xception. In particular, it will:\n",
    "\n",
    "* Resize the image to the 299 x 299 pixels expected by Xception.\n",
    "* Bundle the image in a tensor that represents a batch of a single image.\n",
    "* Call the Xception preprocessing function to the all the pixel normalization steps this network needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.xception import preprocess_input\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "\n",
    "def preprocess_image(img):\n",
    "    img = resize(img, (299, 299, 3), preserve_range=True, mode=\"reflect\", anti_aliasing=True)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return preprocess_input(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through this preprocessing we can obtain predictions using the network. We will use the **decode_predictions** function of Xception to obtain an explanation on which are the top 5 predicted classes for this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.xception import decode_predictions\n",
    "\n",
    "preds = model.predict(preprocess_image(img))\n",
    "print('Predicted:', decode_predictions(preds)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top predicted class is **orange** with very high probability, followed by **lemon** with a small probability. That makes sense!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
    "\n",
    "***\n",
    "\n",
    "<font color=#ad3e26>\n",
    "Using the functions above, write in the cell below the code needed to download a new image, preprocess it, and obtain the network predictions. You can use any image found in the internet! Just look for some image you like and use its URL as input for the <i>download_image</i> function. How well does the network work with the images you have chosen?\n",
    "</font>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with any image URL you want! Here is a photograph of a couple of wolves, but feel free to change it!\n",
    "!wget -O input.jpg \"https://cdn.pixabay.com/photo/2019/12/19/22/48/wolf-4707294_960_720.jpg\"\n",
    "img = np.array(Image.open(\"./input.jpg\"))\n",
    "\n",
    "display(Image.fromarray(img))\n",
    "preds = model.predict(x = preprocess_image(img))\n",
    "print('Predicted:', decode_predictions(preds)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting explanations from the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use an approximation method to request the neural network an explanation about its decisions. In particular, we will use the [Grad-CAM algorithm](http://gradcam.cloudcv.org/), which finds how much each pixel in the image contributes to the predicted classes. In this way, we can highlight those parts of the image that contribute more to the decision.\n",
    "\n",
    "Since the Grad-CAM algorithm is not implemented in Keras, we will need to build it step by a step. Let's see how! First, we define a function that produces a heatmap of activations following the Grad-CAM steps. Note how the function needs to receive the image to analyze, the neural network model, and the names of the last convolutional layer and the classifier layer. These are the layers whose gradients get analyzed in the Grad-CAM algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "from keras import Input, Model\n",
    "import tensorflow as tf\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, classifier_layer_names, top_n=0):\n",
    "    \"\"\"Creates a Grad-CAM heatmap showing hot spots for a given predicted class\n",
    "    We will\n",
    "    Adapted from https://keras.io/examples/vision/grad_cam/#the-gradcam-algorithm\n",
    "    \"\"\"\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer\n",
    "    last_conv_layer = model.get_layer(last_conv_layer_name)\n",
    "    last_conv_layer_model = Model(model.inputs, last_conv_layer.output)\n",
    "\n",
    "    # Second, we create a model that maps the activations of the last conv\n",
    "    # layer to the final class predictions\n",
    "    classifier_input = Input(shape=last_conv_layer.output.shape[1:])\n",
    "    x = classifier_input\n",
    "    for layer_name in classifier_layer_names:\n",
    "        x = model.get_layer(layer_name)(x)\n",
    "    classifier_model = Model(classifier_input, x)\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute activations of the last conv layer and make the tape watch it\n",
    "        last_conv_layer_output = last_conv_layer_model(img_array)\n",
    "        tape.watch(last_conv_layer_output)\n",
    "        # Compute class predictions for single image\n",
    "        preds = classifier_model(last_conv_layer_output)[0]\n",
    "        # Get output for top-k class\n",
    "        idx = tf.math.top_k(preds, k=top_n+1).indices[top_n]\n",
    "        target_class_channel = preds[idx]\n",
    "\n",
    "    # This is the gradient of the target predicted class with regard to\n",
    "    # the output feature map of the last conv layer\n",
    "    grads = tape.gradient(target_class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n",
    "    pooled_grads = pooled_grads.numpy()\n",
    "    for i in range(pooled_grads.shape[-1]):\n",
    "        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n",
    "\n",
    "    # The channel-wise mean of the resulting feature map\n",
    "    # is our heatmap of class activation\n",
    "    heatmap = np.mean(last_conv_layer_output, axis=-1)\n",
    "\n",
    "    # Filter out negative values, as we want to focus on pixels that contribute positively to the class\n",
    "    # Also, for visualization purposes, we will normalize the heatmap between 0 & 1\n",
    "    return np.maximum(heatmap, 0) / np.max(heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain a more appealing visualization we define the following function, which superimposes the heatmap onto the original image, producing a new image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "def superimposed_heatmap(img, heatmap):\n",
    "    \"\"\"Creates a new image superimposing a given heatmap on top of an image\n",
    "    \n",
    "    Adapted from https://keras.io/examples/vision/grad_cam/#the-gradcam-algorithm\n",
    "    \"\"\"\n",
    "    # We rescale heatmap to a range 0-255\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # We use jet colormap to colorize heatmap\n",
    "    jet = cm.get_cmap(\"jet\")\n",
    "\n",
    "    # We use RGB values of the colormap\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "    # We create an image with RGB colorized heatmap\n",
    "    jet_heatmap = image.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = image.img_to_array(jet_heatmap)\n",
    "\n",
    "    # Superimpose the heatmap on original image\n",
    "    superimposed_img = jet_heatmap * 0.4 + img\n",
    "    superimposed_img = image.array_to_img(superimposed_img)\n",
    "    \n",
    "    return superimposed_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we create a function that prints out a full Grad-CAM classification report for a given image. It will display the original image together with Grad-CAM visualizations for the top predicted classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradcam_report(img, model, last_conv_layer_name, classifier_layer_names, top_n=3):\n",
    "    # Obtain predictions\n",
    "    x = preprocess_image(img)\n",
    "    preds = model.predict(x)\n",
    "    decoded_preds = decode_predictions(preds, top=top_n)[0]\n",
    "    \n",
    "    # Compute a grad cam visualization for each one of the top predicted classes\n",
    "    heatmaps = [\n",
    "        make_gradcam_heatmap(x, model, last_conv_layer_name, classifier_layer_names, i)\n",
    "        for i in range(top_n)\n",
    "    ]\n",
    "    img_array = image.img_to_array(img)\n",
    "    superimposed_imgs = [superimposed_heatmap(img_array, heatmap) for heatmap in heatmaps]\n",
    "    \n",
    "    # Show report\n",
    "    display(Image.fromarray(img))\n",
    "    print(\"Original image\")\n",
    "    \n",
    "    for decoded_pred, visual in zip(decoded_preds, superimposed_imgs):\n",
    "        display(visual)\n",
    "        print(f\"{decoded_pred[1]} ({decoded_pred[2]*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it! \n",
    "\n",
    "As **last_conv_layer_name** we should use the name of the latest convolutional layer in the network: in the Xception network this layer is named `block14_sepconv2`. As for the **classifier_layer_names**, for Xception we need to provide both the final pooling layer and the predictions layer, `[\"avg_pool\", \"predictions\"]`\n",
    "\n",
    "If you want to use another neural network you would need to check the model summary to find the name of the appropriate layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display Grad CAM\n",
    "gradcam_report(img, model, \"block14_sepconv2\", [\"avg_pool\", \"predictions\"], top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
    "\n",
    "***\n",
    "\n",
    "<font color=#ad3e26>\n",
    "Use the cell below to obtain explanations for other images of your choice. Can you find images for which the explanations make sense? What about images where the network predicts the correct class, but for which the explanations do not make sense?\n",
    "</font>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Here we use an image of a platypus to try to confuse the neural network, but try something different yourself!\n",
    "!wget -O input.jpg \"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/A_duck_billed_platypus_%28watermole%29._Colour_lithograph_after_Wellcome_V0021174ER.jpg/397px-A_duck_billed_platypus_%28watermole%29._Colour_lithograph_after_Wellcome_V0021174ER.jpg\"\n",
    "\n",
    "img = np.array(Image.open(\"./input.jpg\"))\n",
    "\n",
    "gradcam_report(img, model, \"block14_sepconv2\", [\"avg_pool\", \"predictions\"], top_n=5)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
